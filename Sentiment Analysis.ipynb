{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32813f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/ca/f3/609bb7512cad1f02af13daa23aa433b931da34c502211f29fd47dceff624/spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/7a/05/4a3b5c3043c6d84c00bf0f574d326660702b1c10174fe6b44cef3c3dff08/murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/d7/f6/67babf1439cdd6d46e4e805616bee84981305c80e562320c293712f54034/cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/a8/b3/1a73ba16bab53043fd19dd0a7838ae05c705dccb329404dd4ad5925767f1/preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/c8/43/0ee5796e9bf91eb8e1e032b4ef407e2fbec7cb2e3da9b13f44a2d2f82607/thinc-8.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading thinc-8.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/1b/d7/0800af1a75008b3a6a6a24f3efd165f2d2208076e9b8a4b11b66f16217f3/srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/a8/73/0a9d4e7f6e78ef270e3a4532b17e060a02087590cf615ba9943fd1a283e9/blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.4/488.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.2-cp311-cp311-macosx_11_0_arm64.whl (778 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecdd6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from en-core-web-lg==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8cd429",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2d29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('/Users/vijaykumardevalla/Documents/Unstructured Data Analytics/Assignment 2/restaurant_reviews_az.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a9882f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>1/27/2020 22:59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>4/19/2020 5:33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2/29/2020 19:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>3/14/2020 21:47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>1/17/2020 20:32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text             date  \\\n",
       "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
       "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
       "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
       "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0179fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #load spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def normalize(review, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        review = review.lower()\n",
    "    doc = nlp(review)\n",
    "    lemmatized = list()\n",
    "    for token in doc:\n",
    "        if not remove_stopwords or (remove_stopwords and not token.is_stop):\n",
    "            lemmatized.append(token.lemma_)\n",
    "    return \" \".join(lemmatized)\n",
    "data['processed'] = data['text'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313e4aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    33268\n",
       "0    14879\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9162aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.11.17)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3cf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a366ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ac2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies sentiment analysis to a text\n",
    "def analyze_sentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "\n",
    "# Apply the function to each row in the 'text' column\n",
    "data['sentiment'] = data['processed'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863141de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>1/27/2020 22:59</td>\n",
       "      <td>1</td>\n",
       "      <td>ok , hype have hatch chili burger overrate . o...</td>\n",
       "      <td>{'neg': 0.02, 'neu': 0.652, 'pos': 0.328, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>4/19/2020 5:33</td>\n",
       "      <td>1</td>\n",
       "      <td>pandemic pit stop ice cream .... plain sundae ...</td>\n",
       "      <td>{'neg': 0.169, 'neu': 0.632, 'pos': 0.199, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2/29/2020 19:43</td>\n",
       "      <td>1</td>\n",
       "      <td>lucky soft opening let tell ... good . beer wi...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>3/14/2020 21:47</td>\n",
       "      <td>1</td>\n",
       "      <td>go claim jumper disappoint location different ...</td>\n",
       "      <td>{'neg': 0.069, 'neu': 0.587, 'pos': 0.344, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>1/17/2020 20:32</td>\n",
       "      <td>1</td>\n",
       "      <td>maynard kitchen , time ! hope dinner , sure ...</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.67, 'pos': 0.282, 'com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text             date  \\\n",
       "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
       "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
       "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
       "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
       "\n",
       "   Sentiment                                          processed  \\\n",
       "0          1  ok , hype have hatch chili burger overrate . o...   \n",
       "1          1  pandemic pit stop ice cream .... plain sundae ...   \n",
       "2          1  lucky soft opening let tell ... good . beer wi...   \n",
       "3          1  go claim jumper disappoint location different ...   \n",
       "4          1    maynard kitchen , time ! hope dinner , sure ...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.02, 'neu': 0.652, 'pos': 0.328, 'com...  \n",
       "1  {'neg': 0.169, 'neu': 0.632, 'pos': 0.199, 'co...  \n",
       "2  {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'comp...  \n",
       "3  {'neg': 0.069, 'neu': 0.587, 'pos': 0.344, 'co...  \n",
       "4  {'neg': 0.048, 'neu': 0.67, 'pos': 0.282, 'com...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "003001aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into trainig and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['processed'], data['Sentiment'], test_size=0.2, random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dce54f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>1/27/2020 22:59</td>\n",
       "      <td>1</td>\n",
       "      <td>ok , hype have hatch chili burger overrate . o...</td>\n",
       "      <td>{'neg': 0.02, 'neu': 0.652, 'pos': 0.328, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>4/19/2020 5:33</td>\n",
       "      <td>1</td>\n",
       "      <td>pandemic pit stop ice cream .... plain sundae ...</td>\n",
       "      <td>{'neg': 0.169, 'neu': 0.632, 'pos': 0.199, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2/29/2020 19:43</td>\n",
       "      <td>1</td>\n",
       "      <td>lucky soft opening let tell ... good . beer wi...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>3/14/2020 21:47</td>\n",
       "      <td>1</td>\n",
       "      <td>go claim jumper disappoint location different ...</td>\n",
       "      <td>{'neg': 0.069, 'neu': 0.587, 'pos': 0.344, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>1/17/2020 20:32</td>\n",
       "      <td>1</td>\n",
       "      <td>maynard kitchen , time ! hope dinner , sure ...</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.67, 'pos': 0.282, 'com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text             date  \\\n",
       "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
       "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
       "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
       "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
       "\n",
       "   Sentiment                                          processed  \\\n",
       "0          1  ok , hype have hatch chili burger overrate . o...   \n",
       "1          1  pandemic pit stop ice cream .... plain sundae ...   \n",
       "2          1  lucky soft opening let tell ... good . beer wi...   \n",
       "3          1  go claim jumper disappoint location different ...   \n",
       "4          1    maynard kitchen , time ! hope dinner , sure ...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.02, 'neu': 0.652, 'pos': 0.328, 'com...  \n",
       "1  {'neg': 0.169, 'neu': 0.632, 'pos': 0.199, 'co...  \n",
       "2  {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'comp...  \n",
       "3  {'neg': 0.069, 'neu': 0.587, 'pos': 0.344, 'co...  \n",
       "4  {'neg': 0.048, 'neu': 0.67, 'pos': 0.282, 'com...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc72fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38517, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize, max_features = 1000)\n",
    "#cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize, max_features = 3000, binary = True)\n",
    "X_train_vect = cv.fit_transform(X_train)\n",
    "X_train_vect.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63eb459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>act</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolute  absolutely  accommodate  act  actual  actually  add  \\\n",
       "0     0         0           0            0    0       0         0    0   \n",
       "1     0         0           0            0    0       0         0    0   \n",
       "2     0         0           0            0    0       0         0    0   \n",
       "3     0         0           0            0    0       0         1    0   \n",
       "4     0         0           0            0    0       0         0    0   \n",
       "\n",
       "   addition  additional  ...  write  wrong  year  yelp  yes  yesterday  young  \\\n",
       "0         0           0  ...      0      0     0     0    0          0      0   \n",
       "1         0           0  ...      0      0     1     0    1          0      0   \n",
       "2         0           0  ...      0      0     0     1    0          0      0   \n",
       "3         0           0  ...      0      1     0     0    0          0      0   \n",
       "4         0           0  ...      0      0     0     0    0          0      0   \n",
       "\n",
       "   yum  yummy  zero  \n",
       "0    0      0     0  \n",
       "1    0      0     0  \n",
       "2    0      0     0  \n",
       "3    0      0     0  \n",
       "4    0      0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train_vect.toarray(), columns=cv.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03b3c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to store term statistics\n",
    "num_of_comments = 0\n",
    "unique_word = set() # using the set-type variable since it does not allow duplicates > able to count the number of unique words\n",
    "num_of_token_per_comment = [] # using the list-type varailbe since we want to measure corpus-level statistics (e.g., avg, max, min, median, etc.)\n",
    "num_of_token_per_comment_without_stop_words = []\n",
    "total_number_of_tokens = 0 # in a corpus\n",
    "unique_user = set() # using the set-type variable since it does not allow duplicates > able to count the number of unique users\n",
    "date_list = [] # able to measure the number of comments by day, week, etc.\n",
    "vote_count = 0\n",
    "unique_submission = set() # using the set-type variable since it does not allow duplicates > able to count the number of unique submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af83fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    text = row[\"processed\"]\n",
    "    doc = nlp(text)\n",
    "    num_of_comments += 1\n",
    "\n",
    "    # statistics regarding words\n",
    "    num_of_tokens = len(doc)\n",
    "    total_number_of_tokens += num_of_tokens\n",
    "    token_count_without_stop_words = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_stop is True:\n",
    "            pass\n",
    "        else:\n",
    "            unique_word.add(str(token).lower())\n",
    "            token_count_without_stop_words += 1\n",
    "\n",
    "    num_of_token_per_comment.append(num_of_tokens)\n",
    "    num_of_token_per_comment_without_stop_words.append(token_count_without_stop_words)\n",
    "\n",
    "    \"\"\"\n",
    "    # statistics regarding users\n",
    "    user_id = row[\"user_id\"]\n",
    "    unique_user.add(author_name.lower())\n",
    "\n",
    "    # statistics regarding date\n",
    "    date = row[\"date\"]\n",
    "    date_list.append(date)\n",
    "\n",
    "    # statistics regarding reviews\n",
    "    review_id = row[\"review_id\"]\n",
    "    unique_submission.add(review_id)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8f0f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train_vect, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0cf7116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9630, 1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vect= cv.transform(X_test)\n",
    "X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7baa5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81      2938\n",
      "           1       0.91      0.92      0.92      6692\n",
      "\n",
      "    accuracy                           0.88      9630\n",
      "   macro avg       0.86      0.86      0.86      9630\n",
      "weighted avg       0.88      0.88      0.88      9630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the performance of the model\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test_vect)\n",
    "performance = metrics.classification_report(Y_test,predicted, target_names= ['0', '1'])\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e61958c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_vect, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d06dcd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      2938\n",
      "           1       0.93      0.94      0.93      6692\n",
      "\n",
      "    accuracy                           0.91      9630\n",
      "   macro avg       0.89      0.89      0.89      9630\n",
      "weighted avg       0.91      0.91      0.91      9630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "predicted = clf.predict(X_test_vect)\n",
    "performance = metrics.classification_report(Y_test,predicted, target_names= ['0', '1'])\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a78887c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38517, 800)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "vectorizer = TfidfVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize, max_features = 800)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5526819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>addition</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>al</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accommodate  actually  add  addition  afternoon  ago  \\\n",
       "0   0.0         0.0          0.0  0.000000  0.0       0.0        0.0  0.0   \n",
       "1   0.0         0.0          0.0  0.000000  0.0       0.0        0.0  0.0   \n",
       "2   0.0         0.0          0.0  0.000000  0.0       0.0        0.0  0.0   \n",
       "3   0.0         0.0          0.0  0.129537  0.0       0.0        0.0  0.0   \n",
       "4   0.0         0.0          0.0  0.000000  0.0       0.0        0.0  0.0   \n",
       "\n",
       "   ahead   al  ...  write     wrong      year      yelp       yes  yesterday  \\\n",
       "0    0.0  0.0  ...    0.0  0.000000  0.000000  0.000000  0.000000        0.0   \n",
       "1    0.0  0.0  ...    0.0  0.000000  0.128345  0.000000  0.171716        0.0   \n",
       "2    0.0  0.0  ...    0.0  0.000000  0.000000  0.323791  0.000000        0.0   \n",
       "3    0.0  0.0  ...    0.0  0.124057  0.000000  0.000000  0.000000        0.0   \n",
       "4    0.0  0.0  ...    0.0  0.000000  0.000000  0.000000  0.000000        0.0   \n",
       "\n",
       "   young  yum  yummy  zero  \n",
       "0    0.0  0.0    0.0   0.0  \n",
       "1    0.0  0.0    0.0   0.0  \n",
       "2    0.0  0.0    0.0   0.0  \n",
       "3    0.0  0.0    0.0   0.0  \n",
       "4    0.0  0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91456e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f61bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train_vect, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5fde24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.84      2938\n",
      "           1       0.92      0.94      0.93      6692\n",
      "\n",
      "    accuracy                           0.90      9630\n",
      "   macro avg       0.89      0.88      0.88      9630\n",
      "weighted avg       0.90      0.90      0.90      9630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_lr = LR.predict(X_test_vect)\n",
    "performance_lr = metrics.classification_report(Y_test, predicted_lr, target_names=['0', '1'])\n",
    "print(performance_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1fe8b",
   "metadata": {},
   "source": [
    "# Conclusion for lexicon based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96c783ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.39      0.55      2938\n",
      "           1       0.79      0.98      0.87      6692\n",
      "\n",
      "    accuracy                           0.80      9630\n",
      "   macro avg       0.84      0.69      0.71      9630\n",
      "weighted avg       0.82      0.80      0.77      9630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_predicted = []\n",
    "for text in X_test:\n",
    "  sent= sentiment.polarity_scores(text)\n",
    "  if sent['compound']>0:\n",
    "    v_predicted.append(1)\n",
    "  else:\n",
    "    v_predicted.append(0)\n",
    "v_performance = metrics.classification_report(Y_test,v_predicted, target_names= ['0', '1'])\n",
    "print(v_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be72658",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Model Performance: All three machine learning models demonstrate good performance with accuracy above 0.88. SVC performs the best in terms of accuracy (0.91), followed closely by Logistic Regression (0.90) and Naive Bayes (0.88).\n",
    "\n",
    "Class Imbalance Handling: The class distribution (2938 for class 0 and 6692 for class 1) suggests a class imbalance. Despite this, all models perform relatively well for both classes, with slightly better performance on the majority class (class 1). This is a typical scenario in sentiment analysis tasks.\n",
    "\n",
    "Precision and Recall: SVC and Logistic Regression have similar precision and recall scores, which are marginally better than those of Naive Bayes. This suggests they might be better at minimizing both false positives and false negatives.\n",
    "\n",
    "F1-Score: The F1-score, which balances precision and recall, is consistently higher for class 1 across all models, indicating better performance on the positive class.\n",
    "\n",
    "\n",
    "While lexicon-based sentiment analysis provided a simple and quick method for sentiment classification, machine learning models, as demonstrated by the provided outputs, tend to offer more robust and context-aware sentiment analysis. They yielded higher accuracy and are more adaptable to different types of text and domains. Among the machine learning models, SVC and Logistic Regression show slightly better overall performance compared to Naive Bayes in this particular task, particularly in handling both the positive and negative classes effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa366387",
   "metadata": {},
   "source": [
    "#Input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caaa71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= ['The service is good, but location is hard to find. Sanitation is not very good with old facilities. Food served tasted extremely fishy, making us difficult to even finish it.'\n",
    ",'The restaurant is definitely one of my favorites and of my family as well. I was especially impressed with my visit a few days ago. The place is clean, and you just need to wait for fewer than 10 minutes to get food served. And of course, the food is absolutely delicious!'\n",
    ", 'I appreciated the friendly staff. The food was good, not amazing. The service was not prompt but almost acceptable. A reliable spot for a regular meal, but nothing extraordinary.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "331f90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(text, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b43d3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The service is good, but location is hard to f...</td>\n",
       "      <td>service good , location hard find . sanitation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The restaurant is definitely one of my favorit...</td>\n",
       "      <td>restaurant definitely favorite family . especi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I appreciated the friendly staff. The food was...</td>\n",
       "      <td>appreciate friendly staff . food good , amazin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The service is good, but location is hard to f...   \n",
       "1  The restaurant is definitely one of my favorit...   \n",
       "2  I appreciated the friendly staff. The food was...   \n",
       "\n",
       "                                      text_processed  \n",
       "0  service good , location hard find . sanitation...  \n",
       "1  restaurant definitely favorite family . especi...  \n",
       "2  appreciate friendly staff . food good , amazin...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14313fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['text_processed'] = dataframe['text'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d9c8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_vect = cv.transform(dataframe['text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e3d3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = LR.predict(X3_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b2fd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['predicted_sentiment'] = predicted_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d464495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The service is good, but location is hard to f...</td>\n",
       "      <td>service good , location hard find . sanitation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The restaurant is definitely one of my favorit...</td>\n",
       "      <td>restaurant definitely favorite family . especi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I appreciated the friendly staff. The food was...</td>\n",
       "      <td>appreciate friendly staff . food good , amazin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The service is good, but location is hard to f...   \n",
       "1  The restaurant is definitely one of my favorit...   \n",
       "2  I appreciated the friendly staff. The food was...   \n",
       "\n",
       "                                      text_processed  predicted_sentiment  \n",
       "0  service good , location hard find . sanitation...                    1  \n",
       "1  restaurant definitely favorite family . especi...                    1  \n",
       "2  appreciate friendly staff . food good , amazin...                    1  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5852ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  The service is good, but location is hard to f...   \n",
      "1  The restaurant is definitely one of my favorit...   \n",
      "2  I appreciated the friendly staff. The food was...   \n",
      "\n",
      "                                      text_processed  predicted_sentiment  \\\n",
      "0  service good , location hard find . sanitation...                    1   \n",
      "1  restaurant definitely favorite family . especi...                    1   \n",
      "2  appreciate friendly staff . food good , amazin...                    1   \n",
      "\n",
      "  classification  positive_probability  \n",
      "0        neutral              0.586172  \n",
      "1       positive              0.979902  \n",
      "2       positive              0.982168  \n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment probabilities\n",
    "probabilities = LR.predict_proba(X3_vect)\n",
    "\n",
    "# Define thresholds\n",
    "positive_threshold = 0.7\n",
    "negative_threshold = 0.3\n",
    "\n",
    "# Classify each review\n",
    "classifications = []\n",
    "for prob in probabilities:\n",
    "    if prob[1] >= positive_threshold:\n",
    "        classifications.append('positive')\n",
    "    elif prob[1] <= negative_threshold:\n",
    "        classifications.append('negative')\n",
    "    else:\n",
    "        classifications.append('neutral')\n",
    "\n",
    "# Add classifications to the DataFrame\n",
    "dataframe['classification'] = classifications\n",
    "\n",
    "# Display the DataFrame with classification and probabilities\n",
    "dataframe['positive_probability'] = probabilities[:, 1]\n",
    "print(dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e7021",
   "metadata": {},
   "source": [
    "Review 0: Classified as 'neutral' with a 58.6% positive sentiment probability. This makes sense because the review has mixed sentiments – it mentions good service but also points out negative aspects like location and sanitation. The model's probability suggests a slight positive bias but not strong enough to fully outweigh the negatives.\n",
    "\n",
    "Review 1: Labeled as 'positive', and the model is 98% sure about this. This aligns well with the review's content, which is full of praises for the restaurant. The high probability reflects the clearly positive tone of the review.\n",
    "\n",
    "Review 2: Also marked as 'positive' with a high confidence of 98.2%. While the review does include a minor critique, it primarily expresses satisfaction, particularly with the staff and food quality. The model seems to recognize the overall positive sentiment despite the slight criticism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "710b1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c1aedb",
   "metadata": {},
   "source": [
    "I have taken help from chatgpt to finish this assignment. Although I have taken help, I know what code I am using and why I am using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b66dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
