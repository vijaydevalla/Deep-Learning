{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9645ac",
   "metadata": {},
   "source": [
    "Author Name  - Vijay Kumar Devalla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dadd3b",
   "metadata": {},
   "source": [
    "ASU ID - 1229148936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46398a2",
   "metadata": {},
   "source": [
    "File creation date - Feb 16th, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b716cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf473863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('/Users/vijaykumardevalla/Documents/Unstructured Data Analytics/Assignment 5/restaurant_reviews_az.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26936dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26bf562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>1/27/2020 22:59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>4/19/2020 5:33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2/29/2020 19:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>3/14/2020 21:47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>1/17/2020 20:32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text             date  \\\n",
       "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
       "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
       "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
       "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab12d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48147 entries, 0 to 48146\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_id    48147 non-null  object\n",
      " 1   user_id      48147 non-null  object\n",
      " 2   business_id  48147 non-null  object\n",
      " 3   stars        48147 non-null  int64 \n",
      " 4   useful       48147 non-null  int64 \n",
      " 5   funny        48147 non-null  int64 \n",
      " 6   cool         48147 non-null  int64 \n",
      " 7   text         48147 non-null  object\n",
      " 8   date         48147 non-null  object\n",
      " 9   Sentiment    48147 non-null  int64 \n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece173d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76d5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_id    1000 non-null   object\n",
      " 1   user_id      1000 non-null   object\n",
      " 2   business_id  1000 non-null   object\n",
      " 3   stars        1000 non-null   int64 \n",
      " 4   useful       1000 non-null   int64 \n",
      " 5   funny        1000 non-null   int64 \n",
      " 6   cool         1000 non-null   int64 \n",
      " 7   text         1000 non-null   object\n",
      " 8   date         1000 non-null   object\n",
      " 9   Sentiment    1000 non-null   int64 \n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfddf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVS7do_HBzroiCiymNdxDg</td>\n",
       "      <td>fdFgZQQYQJeEAshH4lxSfQ</td>\n",
       "      <td>sGy67CpJctjeCWClWqonjA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>OK, the hype about having Hatch chili in your ...</td>\n",
       "      <td>1/27/2020 22:59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QP2pSzSqpJTMWOCuUuyXkQ</td>\n",
       "      <td>JBLWSXBTKFvJYYiM-FnCOQ</td>\n",
       "      <td>3w7NRntdQ9h0KwDsksIt5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pandemic pit stop to have an ice cream.... onl...</td>\n",
       "      <td>4/19/2020 5:33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oK0cGYStgDOusZKz9B1qug</td>\n",
       "      <td>2_9fKnXChUjC5xArfF8BLg</td>\n",
       "      <td>OMnPtRGmbY8qH_wIILfYKA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was lucky enough to go to the soft opening a...</td>\n",
       "      <td>2/29/2020 19:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_ABvFCNVLbfOgRg3Pv1KQ</td>\n",
       "      <td>9MExTQ76GSKhxSWnTS901g</td>\n",
       "      <td>V9XlikTxq0My4gE8LULsjw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I've gone to claim Jumpers all over the US and...</td>\n",
       "      <td>3/14/2020 21:47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rd222CrrnXkXukR2iWj69g</td>\n",
       "      <td>LPxuausjvDN88uPr-Q4cQA</td>\n",
       "      <td>CA5BOxKRDPGJgdUQ8OUOpw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you haven't been  to Maynard's kitchen, it'...</td>\n",
       "      <td>1/17/2020 20:32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
       "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
       "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
       "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
       "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       1      1     0   \n",
       "1      5       1      1     1   \n",
       "2      5       1      0     0   \n",
       "3      5       0      0     0   \n",
       "4      4       1      0     0   \n",
       "\n",
       "                                                text             date  \\\n",
       "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
       "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
       "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
       "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
       "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32bb1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    762\n",
       "0    238\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff53fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    33268\n",
       "0    14879\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3c7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f6f208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ad0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852d3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3888dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['text']).toarray()\n",
    "y_tfidf = data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ffab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ANN model\n",
    "model = Sequential([\n",
    "    Dense(2000, activation='relu', input_shape=(1000,)),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322137e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4514/4514 [==============================] - 23s 5ms/step - loss: 0.5261 - accuracy: 0.7330 - val_loss: 0.3346 - val_accuracy: 0.8688\n",
      "Epoch 2/2\n",
      "  38/4514 [..............................] - ETA: 18s - loss: 0.3649 - accuracy: 0.8717"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4514/4514 [==============================] - 22s 5ms/step - loss: 0.2631 - accuracy: 0.8941 - val_loss: 0.2323 - val_accuracy: 0.9099\n",
      "Best Validation Accuracy: 90.99%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_tfidf and y_tfidf are defined earlier and model is already defined and compiled\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_tfidf, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Fit the model and save the history\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=8, callbacks=[checkpoint])\n",
    "\n",
    "# Extract the history of validation accuracy\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Find the maximum validation accuracy achieved\n",
    "max_val_acc = max(val_acc)\n",
    "\n",
    "print(f\"Best Validation Accuracy: {max_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "751e50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Criteria:\n",
      "Positive: Probability > 0.7\n",
      "Neutral: Probability between 0.3 and 0.7\n",
      "Negative: Probability < 0.3\n",
      "\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Review 1: Sentiment Negative with probability 0.18975599110126495\n",
      "Review 2: Sentiment Positive with probability 0.9916536808013916\n",
      "Review 3: Sentiment Neutral with probability 0.5723163485527039\n"
     ]
    }
   ],
   "source": [
    "# Define criteria for classification\n",
    "positive_threshold = 0.7  # Probabilities above this are considered positive\n",
    "negative_threshold = 0.3  # Probabilities below this are considered negative\n",
    "# Probabilities between negative_threshold and positive_threshold are considered neutral\n",
    "\n",
    "# Print criteria for classification\n",
    "print(f\"Classification Criteria:\")\n",
    "print(f\"Positive: Probability > {positive_threshold}\")\n",
    "print(f\"Neutral: Probability between {negative_threshold} and {positive_threshold}\")\n",
    "print(f\"Negative: Probability < {negative_threshold}\\n\")\n",
    "\n",
    "# Define the list of review texts\n",
    "reviews = [\n",
    "    \"The service is good, but location is hard to find. Sanitation is not very good with old facilities. Food served tasted extremely fishy, making us difficult to even finish it.\",\n",
    "    \"The restaurant is definitely one of my favorites and of my family as well. I was especially impressed with my visit a few days ago. The place is clean, and you just need to wait for fewer than 10 minutes to get food served. And of course, the food is absolutely delicious!\",\n",
    "    \"I appreciated the friendly staff. The food was good, not amazing. The service was not prompt but almost acceptable. A reliable spot for a regular meal, but nothing extraordinary.\"\n",
    "]\n",
    "\n",
    "# Vectorize the reviews\n",
    "X_new = vectorizer.transform(reviews).toarray()\n",
    "\n",
    "# Predict sentiments\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Print classification results and reasoning\n",
    "for i, review in enumerate(reviews):\n",
    "    sentiment = 'Neutral'  # Default to neutral\n",
    "    if predictions[i] > positive_threshold:\n",
    "        sentiment = 'Positive'\n",
    "    elif predictions[i] < negative_threshold:\n",
    "        sentiment = 'Negative'\n",
    "    print(f\"Review {i+1}: Sentiment {sentiment} with probability {predictions[i][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "397bd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (6176, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Reload Data\n",
    "\n",
    "reviews = df['text'].values\n",
    "y = df['Sentiment'].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Step 2: Tokenize and Pad Sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "X_padded = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Preparing for Step 3: Load GloVe Embeddings (assuming you've downloaded 'glove.6B.zip' and extracted it)\n",
    "embed_file = '/Users/vijaykumardevalla/Documents/Unstructured Data Analytics/Assignment 5/glove.6B/glove.6B.50d.txt'  # Make sure this path is correct and file is extracted\n",
    "\n",
    "# Load GloVe 300d embeddings\n",
    "embeddings_index = {}\n",
    "with open(embed_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Step 4: Create Embedding Matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Plus one for padding token\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Verification\n",
    "print(\"Shape of embedding matrix:\", embedding_matrix.shape)  # Should be (vocab_size, 300)\n",
    "\n",
    "# At this point, you have:\n",
    "# - X_padded: Tokenized and padded sequences ready for model input\n",
    "# - embedding_matrix: Prepared embedding matrix using GloVe 300d embeddings\n",
    "# - y: Target sentiment labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e05b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85e79eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Enable mixed precision\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f6e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4514/4514 [==============================] - 733s 162ms/step - loss: 0.6255 - accuracy: 0.6916 - val_loss: 0.6202 - val_accuracy: 0.6888\n",
      "Epoch 2/2\n",
      "4514/4514 [==============================] - 946s 210ms/step - loss: 0.6177 - accuracy: 0.6917 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Best Validation Accuracy: 68.88%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined from the previous cell\n",
    "\n",
    "# Enable mixed precision\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=1000, weights=[embedding_matrix], trainable=False),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),  # Reduced complexity\n",
    "    Dense(512, activation='relu'),  # Reduced complexity\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer for better performance\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare callbacks for saving the best model and early stopping\n",
    "checkpoint = ModelCheckpoint('best_ANN_model_optimized.hdf5', monitor='val_accuracy', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=2,  # As specified\n",
    "    batch_size=8,  # As specified\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "# Optional: Print the best validation accuracy\n",
    "val_acc = max(history.history['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a623863",
   "metadata": {},
   "source": [
    "The analysis showcases a pronounced disparity in the efficacy between TF-IDF and ANN models, with TF-IDF achieving a remarkable accuracy of 90.99%, significantly surpassing the ANN model's 68.88%. This performance differential underscores the effectiveness and efficiency of TF-IDF in specific analytical scenarios, particularly in tasks that benefit from a keyword-focused approach and do not necessitate extensive computational resources. Such findings emphasize the criticality of selecting an appropriate modeling approach tailored to the task's unique demands, computational availability, and dataset characteristics.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f715616",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a67c3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "94/94 [==============================] - 31s 328ms/step - loss: 0.5566 - accuracy: 0.7613 - val_loss: 0.5581 - val_accuracy: 0.7480\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 31s 333ms/step - loss: 0.5177 - accuracy: 0.7667 - val_loss: 0.5491 - val_accuracy: 0.7480\n",
      "Best Validation Accuracy: 74.80%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_padded, y, and embedding_matrix are prepared as per the previous instructions\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, weights=[embedding_matrix], input_length=50, trainable=False),\n",
    "    SimpleRNN(2000, return_sequences=True, activation='relu'),  # First RNN layer with return_sequences=True\n",
    "    SimpleRNN(1000, activation='relu'),  # Second RNN layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model architecture\n",
    "#tf.keras.utils.plot_model(model, to_file='RNN_model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Prepare callbacks for saving the best model based on validation accuracy and for early stopping\n",
    "checkpoint = ModelCheckpoint('best_RNN_model.hdf5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=2, batch_size=8, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Print the best validation accuracy\n",
    "val_acc = max(history.history['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72967ecc",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5acd27e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "94/94 [==============================] - 159s 2s/step - loss: 0.5766 - accuracy: 0.7667 - val_loss: 0.5647 - val_accuracy: 0.7480\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vijaykumardevalla/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 159s 2s/step - loss: 0.5469 - accuracy: 0.7667 - val_loss: 0.5632 - val_accuracy: 0.7480\n",
      "Best Validation Accuracy: 74.80%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_padded, y, and embedding_matrix are prepared as per the previous instructions\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, weights=[embedding_matrix], input_length=50, trainable=False),\n",
    "    LSTM(2000, return_sequences=True, activation='relu'),  # First LSTM layer with return_sequences=True\n",
    "    LSTM(1000, activation='relu'),  # Second LSTM layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model architecture\n",
    "#tf.keras.utils.plot_model(model, to_file='LSTM_model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Prepare callbacks for saving the best model based on validation accuracy and for early stopping\n",
    "checkpoint = ModelCheckpoint('best_LSTM_model.hdf5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=2, batch_size=8, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Print the best validation accuracy\n",
    "val_acc = max(history.history['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea2250",
   "metadata": {},
   "source": [
    "# Compare and comment on your observations on the performance of different deep learning models (i.e., ANN vs. RNN vs. LSTM).\n",
    "\n",
    "Results indicate that RNN and LSTM models, both achieving a best validation accuracy of 74.80%, outperform the ANN model with word embeddings, which reached 68.88% accuracy. This suggests that the sequential processing capabilities of RNNs and LSTMs, which allow them to better understand the context and dependencies within the text data, provide a clear advantage for tasks requiring nuanced understanding of language, such as sentiment analysis. Despite the simplicity and lower computational demands of the ANN, its performance lag suggests that capturing temporal dynamics in text data is crucial for higher accuracy in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d456f",
   "metadata": {},
   "source": [
    "I took help from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff6b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
